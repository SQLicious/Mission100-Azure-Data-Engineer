{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7e83ba-34cd-4f9e-8fb5-eb5ec719cf28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:blue\">\n",
    " <h2> Databricks-Assignment 2 (Basic Data Cleaning Transformation)\n",
    "</span>\n",
    "  <h5>\n",
    "    <span style=\"color:red\">\n",
    "<b>Author: Deepak Goyal <br>\n",
    "   <a> Assignment completed by : Roopmathi Gunna </a><br>\n",
    "   Intial date ran: 12-14-2023 <br>\n",
    "   Written and executed on DBR 12.2 in DB Community edition\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd2ff1c4-8f5c-4652-8172-8bac6dd9d1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- creationDate: timestamp (nullable = true)\n |-- bonus: double (nullable = true)\n\n+---+----------+---------+--------------------+------+------+-------------------+--------+\n| id|first_name|last_name|               email|gender|salary|       creationDate|   bonus|\n+---+----------+---------+--------------------+------+------+-------------------+--------+\n|  1|    Valene|   Ingley|vingley0@livejour...|Female| 44104|1957-09-09 16:44:40|    null|\n|  2|  Lynnelle|    Hurll| lhurll1@answers.com|Female|112411|1907-05-10 17:38:56|    null|\n|  3|   Miranda|    Train|   mtrain2@imgur.com|Female| 91073|1941-01-24 16:05:23|12875.54|\n|  4|    Dulsea|     Foss|dfoss3@dagondesig...|Female|193291|1942-05-09 20:59:39|    null|\n|  5|    Anatol|  Dunklee| adunklee4@google.de|  Male| 22175|1950-07-26 16:28:00| 1432.12|\n+---+----------+---------+--------------------+------+------+-------------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Import the employee data file and becessary libraries\n",
    "\n",
    "input = \"/FileStore/Azurelib files/EmployeeData_Assignment2_input.csv\"\n",
    "df = spark.read.csv(input,header = True, inferSchema = True)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c53c84-5d19-4115-a775-314575ce42fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> 1. Find the count of duplicate employee records in the input file (based on id)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4e6162-dd1d-497a-b403-121b7b3ad8da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of duplicate employee records based on id: 1\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "\n",
    "duplicate_count = df.groupBy(\"id\").count().filter(col(\"count\") > 1).count()\n",
    "print(\"Count of duplicate employee records based on id:\", duplicate_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8364e840-c519-4ac5-a1bf-b9b89b994321",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b> 2. Find out how many records have Gender value missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0185598-138a-4fd1-a503-3e1a0c301712",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with missing Gender value: 28\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "gender_missing_count = df.filter(col(\"gender\").isNull()).count()\n",
    "print(\"Number of records with missing Gender value:\", gender_missing_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14772c70-beff-49d9-828a-724188863639",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<b> 3. Are there any missing values in the \"bonus\" field? If so, filled them defualt bonus 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5233f351-ed0d-4c5d-8c49-b382053a6e91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows missing values in the 'bonus' field: 0\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "\n",
    "missing_bonus_count = df.filter(col(\"bonus\").isNull()).count()\n",
    "print(\"Rows missing values in the 'bonus' field:\", missing_bonus_count)\n",
    "\n",
    "# Filling missing values in the \"bonus\" field with the default value 100\n",
    "df = df.na.fill({\"bonus\": 100})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae0fe03-6339-4dac-9a18-1a5a1da78612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<b> 4. Are there any employees with negative salary or bonus amounts in the input file? If so, how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf0728d-c9ff-4d5f-9cb4-00d7a34bb988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of employees with negative salary or bonus amounts: 0\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "\n",
    "negative_amount_count = df.filter((col(\"salary\") < 0) | (col(\"bonus\") < 0)).count()\n",
    "print(\"Number of employees with negative salary or bonus amounts:\", negative_amount_count)\n",
    "\n",
    "#2nd approach using Where \n",
    "negative_amount_count = df.where((col(\"salary\") < 0) | (col(\"bonus\") < 0)).count()\n",
    "print(\"Number of employees with negative salary or bonus amounts:\", negative_amount_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e58642b-6aff-4351-92a0-cea48bfdc948",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<b> 5. Replace all the null/emtpy value in email column with admin@azurelib.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4288320d-f41e-4719-99a6-93fe790beacb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email replaced with default for 115 rows\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "df = df.na.fill({\"email\": \"admin@azurelib.com\"})\n",
    "\n",
    "#check email was fixed\n",
    "rows_fixed = df.where(col(\"email\") == \"admin@azurelib.com\").count()\n",
    "print(\"email replaced with default for\",rows_fixed,\"rows\")                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39815e9-f12a-497e-a658-fc9f1c98d271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<b> 6. Remove all the records where any record has any null values. Find out the total count of the records now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1035982-814b-43ff-b9df-55e89c658f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of records after removing null values: 686\n"
     ]
    }
   ],
   "source": [
    "# Write your code using the Spark Function\n",
    "cleaned_df = df.na.drop()\n",
    "total_records_after_cleaning = cleaned_df.count()\n",
    "print(\"Total count of records after removing null values:\", total_records_after_cleaning)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks-Assignment2 (Basic Data Cleaning Transformation)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
